{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we embark on a journey to analyze sentiment in tweets related to Apple and Google products. Our goals are to understand the dataset's structure, clean and preprocess the data, and prepare it for a machine learning model that predicts sentiment based on tweet content. Let's start by loading the dataset and performing an exploratory data analysis (EDA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "First, we load the dataset to understand its basic structure, including the number of entries, columns, and types of data it contains. This initial exploration is crucial for planning our preprocessing steps.\n",
    "\n",
    "The dataset contains three columns:\n",
    "- `tweet_text`: The text of the tweet.\n",
    "- `emotion_in_tweet_is_directed_at`: The product or brand the tweet is directed at (e.g., iPhone, iPad, Google).\n",
    "- `is_there_an_emotion_directed_at_a_brand_or_product`: The sentiment of the tweet (e.g., Positive emotion, Negative emotion).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (9093, 3)\n",
      "Data types:\n",
      " tweet_text                                            object\n",
      "emotion_in_tweet_is_directed_at                       object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/judge-1377884607_tweet_product_company.csv'\n",
    "tweets_df = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "print(\"Dataset shape:\", tweets_df.shape)\n",
    "print(\"Data types:\\n\", tweets_df.dtypes)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Analysis\n",
    "Identifying and handling missing values is crucial since they can significantly impact the performance of our model.\n",
    "\n",
    "The dataset contains missing values, primarily in the `emotion_in_tweet_is_directed_at` column, with a smaller number in the `tweet_text` column. Since our primary focus is sentiment analysis based on the tweet text, we'll proceed by dropping rows where the tweet text is missing. The `emotion_in_tweet_is_directed_at column` can be ignored for our current purpose, as we are focusing on sentiment, not the specific product mentioned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " tweet_text                                               1\n",
      "emotion_in_tweet_is_directed_at                       5802\n",
      "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = tweets_df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null values in the 'tweet_text' column in the original DataFrame\n",
    "tweets_df = tweets_df.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Distribution\n",
    "Understanding the balance between different sentiment classes helps us gauge the dataset's bias towards certain sentiments. This is important for selecting appropriate modeling and resampling techniques. \n",
    "\n",
    "The `is_there_an_emotion_directed_at_a_brand_or_product` column has four unique labels:\n",
    "\n",
    "- Negative emotion\n",
    "- Positive emotion\n",
    "- No emotion toward brand or product\n",
    "- I can't tell\n",
    "\n",
    "For the sentiment analysis model, we'll focus on positive and negative emotions. We'll treat \"No emotion toward brand or product\" and \"I can't tell\" as neutral or unknown sentiments, which might be excluded from the training to focus the model on distinguishing clearly between positive and negative sentiments.\n",
    "\n",
    "We have 2,978 positive and 570 negative sentiment tweets. This imbalance in the dataset could influence the performance of our model, making it potentially biased towards predicting positive sentiments. We'll need to keep this in mind during the modeling phase, possibly using techniques like oversampling the minority class, undersampling the majority class, or adjusting the class weights in the model training process to address this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution:\n",
      " No emotion toward brand or product    5388\n",
      "Positive emotion                      2978\n",
      "Negative emotion                       570\n",
      "I can't tell                           156\n",
      "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_distribution = tweets_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n",
    "print(\"Sentiment distribution:\\n\", sentiment_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Length Analysis\n",
    "Analyzing the length of tweets can reveal insights about the dataset's variability. It helps us decide if we need to normalize text lengths through padding or truncation during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9092.000000\n",
       "mean      104.962275\n",
       "std        27.187640\n",
       "min        11.000000\n",
       "25%        86.000000\n",
       "50%       109.000000\n",
       "75%       126.000000\n",
       "max       178.000000\n",
       "Name: text_length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['text_length'] = tweets_df['tweet_text'].apply(lambda x: len(str(x)))\n",
    "tweets_df['text_length'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Words Analysis\n",
    "Identifying the most frequently occurring words will help us understand common themes and refine our list of stopwords. High-frequency words with little sentiment value might be removed to improve model accuracy.\n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johns\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('#', 15875),\n",
       " ('sxsw', 9516),\n",
       " ('@', 7194),\n",
       " ('mention', 7124),\n",
       " ('.', 5506),\n",
       " ('the', 4424),\n",
       " ('link', 4313),\n",
       " ('}', 4298),\n",
       " ('{', 4296),\n",
       " ('to', 3586),\n",
       " (',', 3533),\n",
       " ('at', 3102),\n",
       " ('rt', 2962),\n",
       " (';', 2800),\n",
       " ('&', 2707),\n",
       " ('google', 2595),\n",
       " ('for', 2545),\n",
       " ('ipad', 2446),\n",
       " ('!', 2398),\n",
       " ('a', 2312)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "all_words = ' '.join(tweets_df['tweet_text'].dropna()).lower()\n",
    "all_words_tokenized = word_tokenize(all_words)\n",
    "\n",
    "word_counts = Counter(all_words_tokenized)\n",
    "word_counts.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Characters and URLs Analysis\n",
    "Tweets often contain special characters, emojis, and URLs that may not contribute to sentiment analysis. Deciding whether to keep or remove these can impact preprocessing steps and ultimately model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters counts: [('#', 15875), ('.', 8382), ('@', 7194), ('}', 4298), ('{', 4296), (',', 3558), (\"'\", 2903), (';', 2800), ('&', 2707), ('-', 2438)]\n",
      "Number of URLs: 44\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "special_chars = tweets_df['tweet_text'].apply(lambda x: re.findall(r'[^\\w\\s]', str(x)))\n",
    "special_chars_counts = Counter([item for sublist in special_chars for item in sublist])\n",
    "print(\"Special characters counts:\", special_chars_counts.most_common(10))\n",
    "\n",
    "urls_counts = tweets_df['tweet_text'].apply(lambda x: len(re.findall(r\"http\\S+|www\\S+|https\\S+\", str(x)))).sum()\n",
    "print(\"Number of URLs:\", urls_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Sentiment Labels\n",
    "For our model to understand the sentiment labels, we need to encode them into a numerical format. In our case, we'll encode positive sentiment as 1 and negative sentiment as 0. This step is crucial for the subsequent modeling process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {\n",
    "    'Positive emotion': 1,\n",
    "    'Negative emotion': 0\n",
    "}\n",
    "\n",
    "# Update the mapping based on your dataset's sentiment labels\n",
    "tweets_df['sentiment_label'] = tweets_df['is_there_an_emotion_directed_at_a_brand_or_product'].map(sentiment_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 5 columns):\n",
      " #   Column                                              Non-Null Count  Dtype  \n",
      "---  ------                                              --------------  -----  \n",
      " 0   tweet_text                                          9092 non-null   object \n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object \n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9092 non-null   object \n",
      " 3   text_length                                         9092 non-null   int64  \n",
      " 4   sentiment_label                                     3548 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 426.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "After cleaning the text, we need to address missing values. Missing sentiment labels or texts could mislead our model training. Depending on the context, we might choose to fill them with a placeholder value or remove those entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3548 entries, 0 to 9088\n",
      "Data columns (total 5 columns):\n",
      " #   Column                                              Non-Null Count  Dtype  \n",
      "---  ------                                              --------------  -----  \n",
      " 0   tweet_text                                          3548 non-null   object \n",
      " 1   emotion_in_tweet_is_directed_at                     3191 non-null   object \n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  3548 non-null   object \n",
      " 3   text_length                                         3548 non-null   int64  \n",
      " 4   sentiment_label                                     3548 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 166.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with any missing sentiment labels or cleaned tweets\n",
    "tweets_df = tweets_df.dropna(subset=['sentiment_label'])\n",
    "tweets_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data\n",
    "Before training our model, we'll split the dataset into training and testing sets. This allows us to train our model on one subset of the data and then test its performance on unseen data, providing a better evaluation of its real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets_df['tweet_text'],\n",
    "    tweets_df['sentiment_label'],\n",
    "    stratify=tweets_df['sentiment_label'],\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text Data (BorA TTSplit?)\n",
    "The next step in our preprocessing is to clean the text data. This involves removing URLs, mentions, special characters, and numbers. These elements are generally not useful for sentiment analysis and can introduce noise into our model. Cleaning helps in focusing on the meaningful content of the tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johns\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johns\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johns\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003    someone buy ipad v1 wait line get v2 tomorrow ...\n",
       "8981    think effing hubby line ipad someone point tow...\n",
       "7351    smart company rt mention rumor apple opening t...\n",
       "7483                   google map mobile look awesomesxsw\n",
       "1642    mention mention wishing excellent day sxsw tod...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # Lowercase the text\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    tweet = re.sub(r'@\\w+|#\\w+', '', tweet)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Remove numeric characters\n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    clean_tweet = ' '.join(tokens)\n",
    "    \n",
    "    return clean_tweet\n",
    "\n",
    "\n",
    "X_train = X_train.apply(clean_tweet)\n",
    "X_test = X_test.apply(clean_tweet)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_tweet(tweet):\n",
    "#     tweet = str(tweet).lower() #Lower case everything\n",
    "#     tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)  # Remove URLs\n",
    "#     tweet = re.sub(r'\\@\\w+|\\#', '', tweet)  # Remove mentions and keep hashtags\n",
    "#     tweet = re.sub(r'\\d+', '', tweet)  # Remove numbers\n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove punctuation\n",
    "#     return tweet\n",
    "\n",
    "# # tweets_df['cleaned_tweet_text'] = tweets_df['tweet_text'].apply(clean_tweet)\n",
    "\n",
    "# X_train = X_train.apply(clean_tweet)\n",
    "# X_test = X_test.apply(clean_tweet)\n",
    "\n",
    "\n",
    "# # def preprocess_text(text):\n",
    "# #     # Tokenize text\n",
    "# #     tokens = word_tokenize(text)\n",
    "# #     # Remove punctuation\n",
    "# #     tokens = [token for token in tokens if token not in string.punctuation]\n",
    "# #     # Convert tokens to lowercase\n",
    "# #     tokens = [token.lower() for token in tokens]\n",
    "# #     # Lemmatize tokens\n",
    "# #     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "# #     # Remove stop words\n",
    "# #     stop_words = set(stopwords.words('english'))\n",
    "# #     tokens = [token for token in tokens if token not in stop_words]\n",
    "# #     # Join tokens back into a single string\n",
    "# #     preprocessed_text = ' '.join(tokens)\n",
    "# #     return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7003    someone buy ipad v1 wait line get v2 tomorrow ...\n",
       "8981    think effing hubby line ipad someone point tow...\n",
       "7351    smart company rt mention rumor apple opening t...\n",
       "7483                   google map mobile look awesomesxsw\n",
       "1642    mention mention wishing excellent day sxsw tod...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leminization (BorA TTSplit?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# # Lemmatization function\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# def lemmatize_text(text):\n",
    "#     return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# # Apply cleaning and lemmatization\n",
    "# X_train = X_train.apply(lemmatize_text)\n",
    "# X_test = X_test.apply(lemmatize_text)\n",
    "\n",
    "# # Or \n",
    "\n",
    "# # tweets_df['cleaned_tweet_text'] = tweets_df['tweet_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7003    someone buy ipad v1 wait line get v2 tomorrow ...\n",
       "8981    think effing hubby line ipad someone point tow...\n",
       "7351    smart company rt mention rumor apple opening t...\n",
       "7483                   google map mobile look awesomesxsw\n",
       "1642    mention mention wishing excellent day sxsw tod...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization (TF-IDF) (BorA TTSplit?)\n",
    "Machine Learning models require numerical input, so we convert our cleaned text data into a numerical format using TF-IDF (Term Frequency-Inverse Document Frequency). This technique reflects how important a word is to a document in a collection, helping us to weigh the terms accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=10000)\n",
    "# X_tfidf = tfidf_vectorizer.fit_transform(tweets_df['cleaned_tweet_text'])\n",
    "\n",
    "# or\n",
    "# Vectorization with TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=10000, )\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance\n",
    "Our EDA revealed a class imbalance in the sentiment labels. To address this, we can use oversampling for the minority class to help in preventing our model from being biased towards the more frequent class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4764, 5203),\n",
       " 0.0    2382\n",
       " 1.0    2382\n",
       " Name: sentiment_label, dtype: int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#Initialize the RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "#Resample the training data\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Check the balance of the resampled data\n",
    "resampled_balance = pd.Series(y_train_resampled).value_counts()\n",
    "\n",
    "X_train_resampled.shape, resampled_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy Mean  Accuracy Std\n",
      "0  Multinomial Naive Bayes       0.915195      0.010337\n",
      "1   Complement Naive Bayes       0.915195      0.010337\n",
      "2      Logistic Regression       0.920442      0.008823\n",
      "3            Random Forest       0.971453      0.001218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Assuming you've already defined your X (features) and y (target) from the cleaned data\n",
    "\n",
    "# List to keep track of model performance\n",
    "model_performance = []\n",
    "\n",
    "# Function to evaluate a model and add its performance to our list\n",
    "def evaluate_model(model, model_name, X, y, cv=5):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    model_performance.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy Mean': scores.mean(),\n",
    "        'Accuracy Std': scores.std()\n",
    "    })\n",
    "\n",
    "# Example models to evaluate\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define pipelines for different models\n",
    "nb_pipeline = Pipeline([('clf', MultinomialNB())])\n",
    "cnb_pipeline = Pipeline([('clf', ComplementNB())])\n",
    "lr_pipeline = Pipeline([('clf', LogisticRegression(random_state=42))])\n",
    "rf_pipeline = Pipeline([('clf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Evaluate models\n",
    "evaluate_model(nb_pipeline, 'Multinomial Naive Bayes', X_train_resampled, y_train_resampled)\n",
    "evaluate_model(cnb_pipeline, 'Complement Naive Bayes', X_train_resampled, y_train_resampled)\n",
    "evaluate_model(lr_pipeline, 'Logistic Regression', X_train_resampled, y_train_resampled)\n",
    "evaluate_model(rf_pipeline, 'Random Forest', X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Convert the performance list to a DataFrame\n",
    "performance_df = pd.DataFrame(model_performance)\n",
    "\n",
    "# Display the performance DataFrame\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  CV Train Accuracy Mean  CV Test Accuracy Mean  \\\n",
      "0  Multinomial Naive Bayes                0.962479               0.915195   \n",
      "1   Complement Naive Bayes                0.962741               0.915195   \n",
      "2      Logistic Regression                0.963948               0.920442   \n",
      "3            Random Forest                1.000000               0.971453   \n",
      "\n",
      "   CV Test Accuracy Std  \n",
      "0              0.010337  \n",
      "1              0.010337  \n",
      "2              0.008823  \n",
      "3              0.001218  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Assuming you've already defined your X (features) and y (target) from the cleaned data\n",
    "\n",
    "# List to keep track of model performance\n",
    "model_performance = []\n",
    "\n",
    "# Function to evaluate a model and add its performance to our list\n",
    "def evaluate_model(model, model_name, X, y, cv=5):\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring='accuracy', return_train_score = True)\n",
    "    model_performance.append({\n",
    "        'Model': model_name,\n",
    "        'CV Train Accuracy Mean': np.mean(scores['train_score']),\n",
    "        'CV Test Accuracy Mean': np.mean(scores['test_score']),\n",
    "        'CV Test Accuracy Std': np.std(scores['test_score'])\n",
    "    })\n",
    "\n",
    "# Example models to evaluate\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define pipelines for different models\n",
    "nb_pipeline = Pipeline([('clf', MultinomialNB())])\n",
    "cnb_pipeline = Pipeline([('clf', ComplementNB())])\n",
    "lr_pipeline = Pipeline([('clf', LogisticRegression(random_state=42))])\n",
    "rf_pipeline = Pipeline([('clf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "# Evaluate models\n",
    "evaluate_model(nb_pipeline, 'Multinomial Naive Bayes', X_train_resampled, y_train_resampled)\n",
    "evaluate_model(cnb_pipeline, 'Complement Naive Bayes', X_train_resampled, y_train_resampled)\n",
    "evaluate_model(lr_pipeline, 'Logistic Regression', X_train_resampled, y_train_resampled)\n",
    "evaluate_model(rf_pipeline, 'Random Forest', X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Convert the performance list to a DataFrame\n",
    "performance_df = pd.DataFrame(model_performance)\n",
    "\n",
    "# Display the performance DataFrame\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8661971830985915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Random Forest pipeline on the entire training dataset\n",
    "nb_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set using the Random Forest pipeline\n",
    "y_pred_test = nb_pipeline.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8661971830985915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Random Forest pipeline on the entire training dataset\n",
    "cnb_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set using the Random Forest pipeline\n",
    "y_pred_test = cnb_pipeline.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8535211267605634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Random Forest pipeline on the entire training dataset\n",
    "lr_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set using the Random Forest pipeline\n",
    "y_pred_test = lr_pipeline.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8788732394366198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Random Forest pipeline on the entire training dataset\n",
    "rf_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set using the Random Forest pipeline\n",
    "y_pred_test = rf_pipeline.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best Parameters: {'clf__criterion': 'entropy', 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__n_estimators': 500}\n",
      "Best Score: 0.984676430908145\n",
      "Accuracy of Best Model: 0.8830985915492958\n",
      "Classification Report of Best Model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.36      0.50       114\n",
      "         1.0       0.89      0.98      0.93       596\n",
      "\n",
      "    accuracy                           0.88       710\n",
      "   macro avg       0.85      0.67      0.72       710\n",
      "weighted avg       0.88      0.88      0.86       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'clf__criterion': ['gini', 'entropy']  # Example addition\n",
    "    }\n",
    "\n",
    "# Create a GridSearchCV object with the Random Forest pipeline and the parameter grid\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to your data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_best = grid_search.predict(X_test_tfidf)  # Make sure to transform your test set features as needed\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Accuracy of Best Model:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Classification Report of Best Model:\\n\", classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Define the parameter grid for Logistic Regression\n",
    "# lr_param_grid = {\n",
    "#     'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 200, 300, 400],  # Regularization parameter\n",
    "#     'clf__penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "#     'clf__solver': ['liblinear', 'saga'],  # Optimization algorithm\n",
    "#     'clf__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000]  # Maximum number of iterations\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object with the Logistic Regression pipeline and the parameter grid\n",
    "# lr_grid_search = GridSearchCV(lr_pipeline, lr_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# # Fit GridSearchCV to your data\n",
    "# lr_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters (Logistic Regression):\", lr_grid_search.best_params_)\n",
    "# print(\"Best Score (Logistic Regression):\", lr_grid_search.best_score_)\n",
    "\n",
    "# # Predict using the best Logistic Regression model\n",
    "# y_pred_lr_best = lr_grid_search.predict(X_test_tfidf)  # Make sure to transform your test set features as needed\n",
    "\n",
    "# # Evaluate the best Logistic Regression model\n",
    "# print(\"Accuracy of Best Model (Logistic Regression):\", accuracy_score(y_test, y_pred_lr_best))\n",
    "# print(\"Classification Report of Best Model (Logistic Regression):\\n\", classification_report(y_test, y_pred_lr_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Best Parameters (Logistic Regression): {'clf__C': 300, 'clf__max_iter': 1500, 'clf__penalty': 'l1', 'clf__solver': 'saga'}\n",
      "Best Score (Logistic Regression): 0.9615863659209749\n",
      "Accuracy of Best Model (Logistic Regression): 0.8873239436619719\n",
      "Classification Report of Best Model (Logistic Regression):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.58      0.62       114\n",
      "         1.0       0.92      0.95      0.93       596\n",
      "\n",
      "    accuracy                           0.89       710\n",
      "   macro avg       0.80      0.76      0.78       710\n",
      "weighted avg       0.88      0.89      0.88       710\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johns\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "lr_param_grid = {\n",
    "'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 200, 300, 400],  # Regularization parameter\n",
    "    'clf__penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'clf__solver': ['liblinear', 'saga'],  # Optimization algorithm\n",
    "    'clf__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with the Logistic Regression pipeline and the parameter grid\n",
    "lr_grid_search = GridSearchCV(lr_pipeline, lr_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to your data\n",
    "lr_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters (Logistic Regression):\", lr_grid_search.best_params_)\n",
    "print(\"Best Score (Logistic Regression):\", lr_grid_search.best_score_)\n",
    "\n",
    "# Predict using the best Logistic Regression model\n",
    "y_pred_lr_best = lr_grid_search.predict(X_test_tfidf)  # Make sure to transform your test set features as needed\n",
    "\n",
    "# Evaluate the best Logistic Regression model\n",
    "print(\"Accuracy of Best Model (Logistic Regression):\", accuracy_score(y_test, y_pred_lr_best))\n",
    "print(\"Classification Report of Best Model (Logistic Regression):\\n\", classification_report(y_test, y_pred_lr_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters (Multinomial Naive Bayes): {'clf__alpha': 0.1, 'clf__fit_prior': False}\n",
      "Best Score (Multinomial Naive Bayes): 0.9378645057183419\n",
      "Accuracy of Best Model (Multinomial Naive Bayes): 0.8746478873239436\n",
      "Classification Report of Best Model (Multinomial Naive Bayes):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.62      0.61       114\n",
      "         1.0       0.93      0.92      0.93       596\n",
      "\n",
      "    accuracy                           0.87       710\n",
      "   macro avg       0.77      0.77      0.77       710\n",
      "weighted avg       0.88      0.87      0.88       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for Multinomial Naive Bayes\n",
    "nb_param_grid = {\n",
    "    'clf__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 1.0],  # Additive smoothing parameter\n",
    "    'clf__fit_prior': [True, False] # Whether to learn class prior probabilities\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with the Multinomial Naive Bayes pipeline and the parameter grid\n",
    "nb_grid_search = GridSearchCV(nb_pipeline, nb_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to your data\n",
    "nb_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters (Multinomial Naive Bayes):\", nb_grid_search.best_params_)\n",
    "print(\"Best Score (Multinomial Naive Bayes):\", nb_grid_search.best_score_)\n",
    "\n",
    "# Predict using the best Multinomial Naive Bayes model\n",
    "y_pred_nb_best = nb_grid_search.predict(X_test_tfidf)  # Make sure to transform your test set features as needed\n",
    "\n",
    "# Evaluate the best Multinomial Naive Bayes model\n",
    "print(\"Accuracy of Best Model (Multinomial Naive Bayes):\", accuracy_score(y_test, y_pred_nb_best))\n",
    "print(\"Classification Report of Best Model (Multinomial Naive Bayes):\\n\", classification_report(y_test, y_pred_nb_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters (Multinomial Naive Bayes): {'clf__alpha': 0.1, 'clf__fit_prior': True, 'clf__norm': False}\n",
      "Best Score (Multinomial Naive Bayes): 0.9378645057183419\n",
      "Accuracy of Best Model (Multinomial Naive Bayes): 0.8746478873239436\n",
      "Classification Report of Best Model (Multinomial Naive Bayes):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.62      0.61       114\n",
      "         1.0       0.93      0.92      0.93       596\n",
      "\n",
      "    accuracy                           0.87       710\n",
      "   macro avg       0.77      0.77      0.77       710\n",
      "weighted avg       0.88      0.87      0.88       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for Multinomial Naive Bayes\n",
    "cnb_param_grid = {\n",
    "    'clf__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 1.0],  # Additive smoothing parameter\n",
    "    'clf__fit_prior': [True, False], # Whether to learn class prior probabilities\n",
    "    'clf__norm': [True, False] # Whether to normalize\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with the Multinomial Naive Bayes pipeline and the parameter grid\n",
    "cnb_grid_search = GridSearchCV(cnb_pipeline, cnb_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to your data\n",
    "cnb_grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters (Multinomial Naive Bayes):\", cnb_grid_search.best_params_)\n",
    "print(\"Best Score (Multinomial Naive Bayes):\", cnb_grid_search.best_score_)\n",
    "\n",
    "# Predict using the best Multinomial Naive Bayes model\n",
    "y_pred_nb_best = cnb_grid_search.predict(X_test_tfidf)  # Make sure to transform your test set features as needed\n",
    "\n",
    "# Evaluate the best Multinomial Naive Bayes model\n",
    "print(\"Accuracy of Best Model (Multinomial Naive Bayes):\", accuracy_score(y_test, y_pred_nb_best))\n",
    "print(\"Classification Report of Best Model (Multinomial Naive Bayes):\\n\", classification_report(y_test, y_pred_nb_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary of emojis\n",
    "# emojis= {\n",
    "# '' : '[grumpy]',\n",
    "# '' : '[angry]',\n",
    "# '' : '[cursing]',\n",
    "# '' : '[disappointed]',\n",
    "# '' : '[frustrated]',\n",
    "# '' : '[crying]',\n",
    "# '' : '[facepalmboy]',\n",
    "# '' : '[displeased]',\n",
    "# '' : '[tearful]',\n",
    "# '' : '[spooked]',\n",
    "# '' : '[laughing]',\n",
    "# '' : '[smiling]',\n",
    "# '' : '[silly]',\n",
    "# '' : '[eyeroll]',\n",
    "# '' : '[smirking]',\n",
    "# '' : '[confused]',\n",
    "# '' : '[voting]',\n",
    "# '' : '[stockup]',\n",
    "# '' : '[stockdown]',\n",
    "# '' : '[distressed]',\n",
    "# '' : '[mad]',\n",
    "# '' : '[unhappy]',\n",
    "# '' : '[laughter]',\n",
    "# '' : '[uhoh]',\n",
    "# '' : '[crossarmsblue]',\n",
    "# '' : '[arm]',\n",
    "# '' : '[crossarmspurple]',\n",
    "# '' : '[battery]',\n",
    "# '' : '[bigsmile]',\n",
    "# '' : '[facepalmgirl]',\n",
    "# '' : '[phone]',\n",
    "# '' : '[distressed]',\n",
    "# '' : '[lmao]',\n",
    "# '' : '[raisedeyebrow]',\n",
    "# '' : '[sad]',\n",
    "# '' : '[yikes]'\n",
    "# }\n",
    "\n",
    "# # Create a function to convert emojis to text\n",
    "# def emoji_fixer(x):\n",
    "#     for k in emojis.keys():\n",
    "#         x=x.replace(k, emojis[k])\n",
    "#     return x\n",
    "\n",
    "# # Apply the function to tweets column\n",
    "# neg_tweets['tweet']=neg_tweets['tweet'].apply(emoji_fixer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
